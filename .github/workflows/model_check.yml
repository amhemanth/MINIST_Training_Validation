# Workflow name displayed in GitHub Actions UI
name: Model Testing and Validation

# Define when this workflow will run
on:
  push:
    branches: [ main ]  # Trigger on pushes to main branch
  pull_request:
    branches: [ main ]  # Trigger on PRs targeting main branch

# Environment variables used across jobs
env:
  MAX_PARAMS: 20000      # Maximum allowed model parameters
  MIN_ACCURACY: 98.0     # Minimum required model accuracy
  PYTHON_VERSION: 3.8    # Python version to use
  PYTHONPATH: ${{ github.workspace }}  # Add workspace to Python path
  MIN_COVERAGE: 80       # Minimum required code coverage percentage

# Workflow jobs
jobs:
  # Job 1: Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3.5.3
    - name: Set up Python
      uses: actions/setup-python@v4.7.1
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    - name: Install dependencies  # Install test dependencies
      run: |
        python -m pip install --upgrade pip wheel setuptools
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist coverage
        pip install -e .
    - name: Run unit tests with coverage  # Run tests and generate coverage
      env:
        PYTHONPATH: ${{ github.workspace }}
      run: |
        python -m pytest tests/unit -v -n auto --import-mode=importlib --cov=src
        coverage report --fail-under=${{ env.MIN_COVERAGE }}
        coverage xml
        coverage html
    - name: Upload coverage to Codecov  # Share coverage reports
      uses: codecov/codecov-action@v4
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        files: ./coverage.xml
        flags: unittests
        name: codecov-unit
        fail_ci_if_error: true
    - name: Upload coverage artifacts  # Save coverage reports
      uses: actions/upload-artifact@v4.0.0
      with:
        name: unit-test-coverage
        path: |
          coverage.xml
          htmlcov/
        retention-days: 5

  # Job 2: Integration Tests
  integration-tests:
    needs: unit-tests  # Run after unit-tests job
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3.5.3
    - name: Set up Python
      uses: actions/setup-python@v4.7.1
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist coverage
        pip install -e .
    - name: Run integration tests with coverage
      run: |
        coverage run -m pytest tests/integration -v -n auto
        coverage report --fail-under=${{ env.MIN_COVERAGE }}
        coverage xml
        coverage html
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        files: ./coverage.xml
        flags: integration
        name: codecov-integration
        fail_ci_if_error: true
    - name: Upload coverage artifacts
      uses: actions/upload-artifact@v4.0.0
      with:
        name: integration-test-coverage
        path: |
          coverage.xml
          htmlcov/
        retention-days: 5

  # Job 3: Model Architecture Validation
  model-validation:
    needs: integration-tests  # Run after integration tests
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3.5.3
    - name: Set up Python
      uses: actions/setup-python@v4.7.1
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install torchinfo
        pip install -e .
    - name: Validate model architecture  # Check model parameters
      run: |
        python -c "
        from src.models.mnist_model import Net
        from torchinfo import summary
        model = Net()
        model_stats = summary(model, input_size=(1, 1, 28, 28), verbose=0)
        total_params = sum(p.numel() for p in model.parameters())
        assert total_params < int('${{ env.MAX_PARAMS }}'), f'Model has {total_params} parameters, exceeding limit of ${{ env.MAX_PARAMS }}'
        print(f'Model validation passed: {total_params} parameters')
        "

  # Job 4: Model Performance Testing
  model-performance:
    needs: model-validation  # Run after model validation
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3.5.3
    - name: Set up Python
      uses: actions/setup-python@v4.7.1
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
    - name: Quick training test  # Test model training
      run: |
        python train.py --epochs 1 --batch-size 64 --quick-test
    - name: Upload model artifacts  # Save trained model
      uses: actions/upload-artifact@v4.0.0
      with:
        name: model-checkpoint
        path: checkpoints/
        retention-days: 5 